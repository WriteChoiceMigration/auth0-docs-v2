import json
import os
from pathlib import Path
import frontmatter

# --- CONFIGURA√á√ïES ---
TRANSLATIONS_FILE = 'mapeamento_paginas_corrigido.json'
SOURCE_DIR = 'docs'

def get_url_key_from_path(filepath, source_dir):
    """Converte um caminho de arquivo em uma chave de URL formatada."""
    relative_path = filepath.relative_to(source_dir)
    url_key = '/' + str(relative_path.with_suffix('')).replace(os.path.sep, '/')
    if url_key.endswith('/index'):
        url_key = url_key[:-5] or '/'
    return url_key

def main():
    print("Iniciando o detetive de mapeamento de URLs...")

    # --- PASSO 1: Criar um √≠ndice reverso de T√≠tulo -> URL a partir do JSON ---
    title_to_url_map = {}
    seen_titles = set()
    try:
        with open(TRANSLATIONS_FILE, 'r', encoding='utf-8') as f:
            translations_data = json.load(f)
    except FileNotFoundError:
        print(f"ERRO: Arquivo de tradu√ß√µes '{TRANSLATIONS_FILE}' n√£o encontrado!")
        return

    print("Construindo √≠ndice de t√≠tulos a partir do JSON...")
    for url_key, page_data in translations_data.items():
        english_title = page_data.get('title', {}).get('en')
        if english_title:
            clean_title = english_title.strip().lower()
            if clean_title in seen_titles:
                print(f"  - Aviso: T√≠tulo duplicado no JSON: '{english_title}' (URL: {url_key})")
            else:
                title_to_url_map[clean_title] = url_key
                seen_titles.add(clean_title)
    print(f"√çndice constru√≠do com {len(title_to_url_map)} t√≠tulos √∫nicos.")

    # --- PASSO 2: Ler os arquivos MDX e encontrar correspond√™ncias pelo t√≠tulo ---
    source_path = Path(SOURCE_DIR)
    mdx_files = list(source_path.rglob('*.mdx'))
    print(f"Analisando {len(mdx_files)} arquivos .mdx para encontrar correspond√™ncias...")
    
    correction_map = {}
    found_count = 0
    not_found_count = 0

    for mdx_path in mdx_files:
        # Ignora as pastas de tradu√ß√£o para n√£o se auto-referenciar
        if any(part in ['fr-ca', 'ja-jp'] for part in mdx_path.parts):
            continue

        try:
            post = frontmatter.load(mdx_path)
            mdx_title = post.metadata.get('title')

            if mdx_title:
                clean_mdx_title = mdx_title.strip().lower()
                
                # Procura o t√≠tulo do arquivo no nosso √≠ndice
                correct_url = title_to_url_map.get(clean_mdx_title)
                
                if correct_url:
                    found_count += 1
                    file_url_key = get_url_key_from_path(mdx_path, source_path)
                    
                    # Se a URL do arquivo √© diferente da URL correta do JSON, √© uma corre√ß√£o!
                    if file_url_key != correct_url:
                        correction_map[file_url_key] = correct_url
                else:
                    not_found_count += 1
            else:
                not_found_count += 1
        except Exception:
            # Ignora arquivos que n√£o podem ser lidos
            not_found_count += 1
            continue

    print("\n--- An√°lise Conclu√≠da ---")
    print(f"‚úÖ Correspond√™ncias encontradas pelo t√≠tulo: {found_count}")
    print(f"‚ùå Arquivos sem correspond√™ncia de t√≠tulo: {not_found_count}")
    print(f"üí° Regras de corre√ß√£o sugeridas: {len(correction_map)}")

    # --- PASSO 3: Imprimir o mapa de corre√ß√£o pronto para ser copiado ---
    print("\n\nCOPIE E COLE O BLOCO ABAIXO NO SEU SCRIPT 'tradutor_final.py':")
    print("-" * 60)
    print("URL_CORRECTION_MAP = {")
    
    # Ordena o dicion√°rio para uma sa√≠da mais limpa
    sorted_corrections = sorted(correction_map.items())
    
    for file_url, correct_url in sorted_corrections:
        print(f'    "{file_url}": "{correct_url}",')
        
    print("}")
    print("-" * 60)

if __name__ == '__main__':
    main()